{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Folding Result :\n",
      "\n",
      " ular cobra adalah predator yang sangat mematikan, ia memiliki bisa yang dapat membunuh manusia dalam waktu beberapa menit saja.\n",
      " bunglon berkamuflase untuk menghindar dari serangan predator.\n",
      " anik adalah mahasiswa serba bisa yang sangat terkenal di unisbank, karena ia memiliki banyak waktu luang untuk belajar.\n",
      " mahasiswa dari berbagai kampus berkumpul di simpang lima untuk berdemo menolak kenaikan bbm.\n",
      "\n",
      "\n",
      "\n",
      "Tokenizing Result : \n",
      "\n",
      "['ular', 'cobra', 'adalah', 'predator', 'yang', 'sangat', 'mematikan', 'ia', 'memiliki', 'bisa', 'yang', 'dapat', 'membunuh', 'manusia', 'dalam', 'waktu', 'beberapa', 'menit', 'saja', 'bunglon', 'berkamuflase', 'untuk', 'menghindar', 'dari', 'serangan', 'predator', 'anik', 'adalah', 'mahasiswa', 'serba', 'bisa', 'yang', 'sangat', 'terkenal', 'di', 'unisbank', 'karena', 'ia', 'memiliki', 'banyak', 'waktu', 'luang', 'untuk', 'belajar', 'mahasiswa', 'dari', 'berbagai', 'kampus', 'berkumpul', 'di', 'simpang', 'lima', 'untuk', 'berdemo', 'menolak', 'kenaikan', 'bbm']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string \n",
    "import re #regex library\n",
    "\n",
    "# import word_tokenize from NLTK\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "\n",
    "\n",
    "# sentence input\n",
    "sentence = (\"\\n Ular cobra adalah predator yang sangat mematikan, ia memiliki bisa yang dapat membunuh manusia dalam waktu beberapa menit saja.\"\n",
    "            \"\\n Bunglon berkamuflase untuk menghindar dari serangan predator.\"\n",
    "            \"\\n Anik adalah mahasiswa serba bisa yang sangat terkenal di UNISBANK, karena ia memiliki banyak waktu luang untuk belajar.\"\n",
    "            \"\\n Mahasiswa dari berbagai kampus berkumpul di Simpang Lima untuk berdemo menolak kenaikan BBM.\")\n",
    "\n",
    "# ------ Case Folding --------\n",
    "# gunakan fungsi .lower()\n",
    "lowercase_sentence = sentence.lower()\n",
    "\n",
    "print('Case Folding Result :')\n",
    "print(lowercase_sentence)\n",
    "print('\\n\\n')\n",
    "\n",
    "# ------ Tokenizing ---------\n",
    "#remove angka\n",
    "lowercase_sentence = re.sub(r\"\\d+\", \"\", lowercase_sentence)\n",
    "\n",
    "#remove punctuation\n",
    "lowercase_sentence = lowercase_sentence.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "\n",
    "#remove whitespace leading & trailing\n",
    "lowercase_sentence = lowercase_sentence.strip()\n",
    "\n",
    "#remove multiple whitespace into single whitespace\n",
    "lowercase_sentence = re.sub('\\s+',' ',lowercase_sentence)\n",
    "\n",
    "\n",
    "tokens = nltk.tokenize.word_tokenize(lowercase_sentence)\n",
    "\n",
    "print('Tokenizing Result : \\n') \n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Folding Result : \n",
      "\n",
      "\n",
      " ular cobra adalah predator yang sangat mematikan, ia memiliki bisa yang dapat membunuh manusia dalam waktu beberapa menit saja.\n",
      " bunglon berkamuflase untuk menghindar dari serangan predator.\n",
      " anik adalah mahasiswa serba bisa yang sangat terkenal di unisbank, karena ia memiliki banyak waktu luang untuk belajar.\n",
      " mahasiswa dari berbagai kampus berkumpul di simpang lima untuk berdemo menolak kenaikan bbm.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tokenizing Result : \n",
      "\n",
      "['ular', 'cobra', 'adalah', 'predator', 'yang', 'sangat', 'mematikan', 'ia', 'memiliki', 'bisa', 'yang', 'dapat', 'membunuh', 'manusia', 'dalam', 'waktu', 'beberapa', 'menit', 'saja', 'bunglon', 'berkamuflase', 'untuk', 'menghindar', 'dari', 'serangan', 'predator', 'anik', 'adalah', 'mahasiswa', 'serba', 'bisa', 'yang', 'sangat', 'terkenal', 'di', 'unisbank', 'karena', 'ia', 'memiliki', 'banyak', 'waktu', 'luang', 'untuk', 'belajar', 'mahasiswa', 'dari', 'berbagai', 'kampus', 'berkumpul', 'di', 'simpang', 'lima', 'untuk', 'berdemo', 'menolak', 'kenaikan', 'bbm']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Frequency Tokens : \n",
      "\n",
      "[('yang', 3), ('untuk', 3), ('adalah', 2), ('predator', 2), ('sangat', 2), ('ia', 2), ('memiliki', 2), ('bisa', 2), ('waktu', 2), ('dari', 2), ('mahasiswa', 2), ('di', 2), ('ular', 1), ('cobra', 1), ('mematikan', 1), ('dapat', 1), ('membunuh', 1), ('manusia', 1), ('dalam', 1), ('beberapa', 1), ('menit', 1), ('saja', 1), ('bunglon', 1), ('berkamuflase', 1), ('menghindar', 1), ('serangan', 1), ('anik', 1), ('serba', 1), ('terkenal', 1), ('unisbank', 1), ('karena', 1), ('banyak', 1), ('luang', 1), ('belajar', 1), ('berbagai', 1), ('kampus', 1), ('berkumpul', 1), ('simpang', 1), ('lima', 1), ('berdemo', 1), ('menolak', 1), ('kenaikan', 1), ('bbm', 1)]\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "import re #regex library\n",
    "\n",
    "# import word_tokenize & FreqDist from NLTK\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "\n",
    "# sentence input\n",
    "sentence = (\"\\n Ular cobra adalah predator yang sangat mematikan, ia memiliki bisa yang dapat membunuh manusia dalam waktu beberapa menit saja.\"\n",
    "            \"\\n Bunglon berkamuflase untuk menghindar dari serangan predator.\"\n",
    "            \"\\n Anik adalah mahasiswa serba bisa yang sangat terkenal di UNISBANK, karena ia memiliki banyak waktu luang untuk belajar.\"\n",
    "            \"\\n Mahasiswa dari berbagai kampus berkumpul di Simpang Lima untuk berdemo menolak kenaikan BBM.\")\n",
    "\n",
    "# ------ Case Folding --------\n",
    "# gunakan fungsi .lower()\n",
    "lowercase_sentence = sentence.lower()\n",
    "\n",
    "print('Case Folding Result : \\n')\n",
    "print(lowercase_sentence)\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "# ------ Tokenizing ---------\n",
    "#remove angka\n",
    "lowercase_sentence = re.sub(r\"\\d+\", \"\", lowercase_sentence)\n",
    "\n",
    "#remove punctuation\n",
    "lowercase_sentence = lowercase_sentence.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "\n",
    "#remove whitespace leading & trailing\n",
    "lowercase_sentence = lowercase_sentence.strip()\n",
    "\n",
    "#remove multiple whitespace into single whitespace\n",
    "lowercase_sentence = re.sub('\\s+',' ',lowercase_sentence)\n",
    "\n",
    "\n",
    "tokens = nltk.tokenize.word_tokenize(lowercase_sentence)\n",
    "\n",
    "print('Tokenizing Result : \\n') \n",
    "print(tokens)\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "freq_tokens = nltk.FreqDist(tokens)\n",
    "\n",
    "print('Frequency Tokens : \\n') \n",
    "print(freq_tokens.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['predator', 'memiliki', 'mahasiswa', 'ular', 'cobra', 'mematikan', 'membunuh', 'manusia', 'menit', 'bunglon', 'berkamuflase', 'menghindar', 'serangan', 'anik', 'serba', 'terkenal', 'unisbank', 'luang', 'belajar', 'kampus', 'berkumpul', 'simpang', 'berdemo', 'menolak', 'kenaikan', 'bbm']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# tokenize text\n",
    "freq_tokens\n",
    "\n",
    "# get Indonesian stopword \n",
    "list_stopwords = set(stopwords.words('indonesian'))\n",
    "\n",
    "#remove stopword pada list token\n",
    "tokens_without_stopword = [word for word in freq_tokens if not word in list_stopwords]\n",
    "\n",
    "print(tokens_without_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['predator : predator',\n",
       " 'memiliki : milik',\n",
       " 'mahasiswa : mahasiswa',\n",
       " 'ular : ular',\n",
       " 'cobra : cobra',\n",
       " 'mematikan : mati',\n",
       " 'membunuh : bunuh',\n",
       " 'manusia : manusia',\n",
       " 'menit : menit',\n",
       " 'bunglon : bunglon',\n",
       " 'berkamuflase : kamuflase',\n",
       " 'menghindar : hindar',\n",
       " 'serangan : serang',\n",
       " 'anik : anik',\n",
       " 'serba : serba',\n",
       " 'terkenal : kenal',\n",
       " 'unisbank : unisbank',\n",
       " 'luang : luang',\n",
       " 'belajar : ajar',\n",
       " 'kampus : kampus',\n",
       " 'berkumpul : kumpul',\n",
       " 'simpang : simpang',\n",
       " 'berdemo : demo',\n",
       " 'menolak : tolak',\n",
       " 'kenaikan : naik',\n",
       " 'bbm : bbm']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import Sastrawi package\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# token without stopword\n",
    "list_tokens = tokens_without_stopword\n",
    "\n",
    "# stem\n",
    "output   = [(token + \" : \" + stemmer.stem(token)) for token in list_tokens]\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
